{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Installations"
      ],
      "metadata": {
        "id": "XU_Rxzf9kYLX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Ohomj1REkNg3",
        "outputId": "63252ffd-aa68-4988-c03c-deebcfc228f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas scikit-learn gensim\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Packages"
      ],
      "metadata": {
        "id": "vmgiMiDok-Qf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel"
      ],
      "metadata": {
        "id": "a7pTGYwvk91c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Excel and filtering columns"
      ],
      "metadata": {
        "id": "vtKCk9m7lBDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load both CSV files\n",
        "file_path_2021 = '/content/hatetoxic_2021.csv'\n",
        "file_path_2223 = '/content/hatetoxic_2223.csv'\n",
        "\n",
        "# Read each file into a DataFrame\n",
        "df_2021 = pd.read_csv(file_path_2021)\n",
        "df_2223 = pd.read_csv(file_path_2223)\n",
        "\n",
        "# Keep only the specified columns\n",
        "columns_to_keep = [\n",
        "    'text without punctuation and stopword', 'subreddit_id', 'moderation', 'year', 'month',\n",
        "    'subreddit_name', 'hate_label', 'toxic_label'\n",
        "]\n",
        "df_2021 = df_2021[columns_to_keep]\n",
        "df_2223 = df_2223[columns_to_keep]\n",
        "\n",
        "# Combine both DataFrames\n",
        "df = pd.concat([df_2021, df_2223], ignore_index=True)\n",
        "\n",
        "# Display the first few rows to verify\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_c-LFZcglD-l",
        "outputId": "ec7dbcb9-81ad-4344-df27-220d1167afd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               text without punctuation and stopword subreddit_id  \\\n",
            "0                                 manually suck dust     t5_2qh8c   \n",
            "1  moral high ground easy realise shame entail le...     t5_2qh8c   \n",
            "2    assume area botanical garden dinosaur shit gone     t5_2qh8c   \n",
            "3  want nanny state everything admit mistake shit...     t5_2qh8c   \n",
            "4  making fun short people exactly singaporean gl...     t5_2qh8c   \n",
            "\n",
            "                                          moderation  year  month  \\\n",
            "0  {'collapsed_reason_code': None, 'collapsed_rea...  2021      7   \n",
            "1  {'removal_reason': None, 'collapsed': False, '...  2020      3   \n",
            "2  {'removal_reason': None, 'collapsed': False, '...  2020     10   \n",
            "3  {'collapsed_reason': None, 'author_is_blocked'...  2021      5   \n",
            "4  {'removal_reason': None, 'collapsed': False, '...  2020      1   \n",
            "\n",
            "  subreddit_name hate_label toxic_label  \n",
            "0    r/Singapore   NOT-HATE       toxic  \n",
            "1    r/Singapore       HATE     neutral  \n",
            "2    r/Singapore   NOT-HATE       toxic  \n",
            "3    r/Singapore   NOT-HATE       toxic  \n",
            "4    r/Singapore   NOT-HATE       toxic  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filtering by year"
      ],
      "metadata": {
        "id": "LBrcgRTblF3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the unique years from the filtered DataFrame\n",
        "years = df['year'].unique()\n",
        "\n",
        "# Create separate DataFrames for each year\n",
        "for year in years:\n",
        "    globals()[f'df_{year}'] = df[df['year'] == year]\n",
        "\n",
        "print(df_2020.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGdsHtD1lJAT",
        "outputId": "aaef4231-6f15-40f8-a2aa-cc362fdb7973"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               text without punctuation and stopword subreddit_id  \\\n",
            "1  moral high ground easy realise shame entail le...     t5_2qh8c   \n",
            "2    assume area botanical garden dinosaur shit gone     t5_2qh8c   \n",
            "4  making fun short people exactly singaporean gl...     t5_2qh8c   \n",
            "6  want n scraped study pretty think saf contagio...     t5_2qh8c   \n",
            "7  hougang likely alr counted result close need r...     t5_2qh8c   \n",
            "\n",
            "                                          moderation  year  month  \\\n",
            "1  {'removal_reason': None, 'collapsed': False, '...  2020      3   \n",
            "2  {'removal_reason': None, 'collapsed': False, '...  2020     10   \n",
            "4  {'removal_reason': None, 'collapsed': False, '...  2020      1   \n",
            "6  {'removal_reason': None, 'collapsed': False, '...  2020      3   \n",
            "7  {'removal_reason': None, 'collapsed': False, '...  2020      7   \n",
            "\n",
            "  subreddit_name hate_label toxic_label  \n",
            "1    r/Singapore       HATE     neutral  \n",
            "2    r/Singapore   NOT-HATE       toxic  \n",
            "4    r/Singapore   NOT-HATE       toxic  \n",
            "6    r/Singapore       HATE     neutral  \n",
            "7    r/Singapore       HATE     neutral  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filtering by year and month"
      ],
      "metadata": {
        "id": "xsvEaaBxlJoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame for each month of each year\n",
        "for year in df['year'].unique():\n",
        "    for month in df['month'].unique():\n",
        "        globals()[f'df_{year}_{month}'] = df[(df['year'] == year) & (df['month'] == month)]\n",
        "\n",
        "print(df_2021_5.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHNgX12ZlIBf",
        "outputId": "3eb5bcf9-fd1f-438b-ca4b-dbcc2866c7ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                text without punctuation and stopword subreddit_id  \\\n",
            "3   want nanny state everything admit mistake shit...     t5_2qh8c   \n",
            "9   dickbags everywhere saf allows dickbags rise s...     t5_2qh8c   \n",
            "14                wound side civet poisoned mutilated     t5_2qh8c   \n",
            "15                          looking police state nerd     t5_2qh8c   \n",
            "32  dun believe come help legitimise second statem...     t5_2qh8c   \n",
            "\n",
            "                                           moderation  year  month  \\\n",
            "3   {'collapsed_reason': None, 'author_is_blocked'...  2021      5   \n",
            "9   {'collapsed_reason': None, 'author_is_blocked'...  2021      5   \n",
            "14  {'collapsed_reason': None, 'author_is_blocked'...  2021      5   \n",
            "15  {'collapsed_reason': None, 'author_is_blocked'...  2021      5   \n",
            "32  {'collapsed_reason': None, 'author_is_blocked'...  2021      5   \n",
            "\n",
            "   subreddit_name hate_label toxic_label  \n",
            "3     r/Singapore   NOT-HATE       toxic  \n",
            "9     r/Singapore       HATE       toxic  \n",
            "14    r/Singapore       HATE     neutral  \n",
            "15    r/Singapore   NOT-HATE       toxic  \n",
            "32    r/Singapore       HATE     neutral  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERTopic"
      ],
      "metadata": {
        "id": "M3ak-8iwqFxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "from hdbscan import HDBSCAN\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import re\n",
        "\n",
        "# Initialize embedding model and set HDBSCAN parameters\n",
        "sentence_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=2, cluster_selection_epsilon=0.01)  # Fine-tuned for flexibility\n",
        "\n",
        "# Initialize BERTopic with `nr_topics` set to \"auto\" for natural clustering\n",
        "topic_model = BERTopic(embedding_model=sentence_model, hdbscan_model=hdbscan_model, nr_topics=\"auto\")\n",
        "\n",
        "# Define custom stopwords\n",
        "stopwords = {\"shit\", \"fuck\", \"fucking\", \"good\", \"time\", \"dumb\", \"singapore\", \"dumbass\", \"gong\", \"people\"}\n",
        "\n",
        "# Dictionary to store results for each subreddit\n",
        "subreddit_topics = {}\n",
        "\n",
        "# List of subreddits to process\n",
        "subreddits = df['subreddit_name'].unique()\n",
        "\n",
        "# Loop through each subreddit\n",
        "for subreddit in subreddits:\n",
        "    # Filter data for the current subreddit\n",
        "    df_subreddit = df[df['subreddit_name'] == subreddit]\n",
        "    docs = df_subreddit['text without punctuation and stopword'].tolist()\n",
        "\n",
        "    # Remove custom stopwords from each document\n",
        "    processed_docs = []\n",
        "    for doc in docs:\n",
        "        # Split text into words, filter out stopwords, and join back into a string\n",
        "        filtered_words = [word for word in doc.split() if word.lower() not in stopwords]\n",
        "        processed_doc = \" \".join(filtered_words)\n",
        "        processed_docs.append(processed_doc)\n",
        "\n",
        "    # Fit the model on the processed documents for the current subreddit\n",
        "    topics, probabilities = topic_model.fit_transform(processed_docs)\n",
        "\n",
        "    # Retrieve topic information\n",
        "    topic_info = topic_model.get_topic_info()\n",
        "    top_topics = topic_info[topic_info[\"Topic\"] != -1].head(10)  # Exclude outliers and get top topics\n",
        "\n",
        "    # Initialize list to store keywords for the current subreddit\n",
        "    keyword_list = []\n",
        "\n",
        "    # Display top topics and keywords for the current subreddit\n",
        "    print(f\"\\nTop Topics and Keywords for {subreddit}:\")\n",
        "    for i, topic in enumerate(top_topics['Topic'].tolist(), start=1):\n",
        "        topic_words = topic_model.get_topic(topic)\n",
        "        if topic_words:\n",
        "            top_words = [word[0] for word in topic_words[:10]]\n",
        "            keyword_list.append(top_words)  # Save keywords for each topic in keyword_list\n",
        "            print(f\"Topic {i}: {' | '.join(top_words)}\")\n",
        "        else:\n",
        "            keyword_list.append([])  # Append empty list if no keywords identified\n",
        "            print(f\"Topic {i}: No clear keywords identified\")\n",
        "\n",
        "    # Store the list of keywords for each topic in the dictionary for each subreddit\n",
        "    subreddit_topics[subreddit] = keyword_list\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuM2TZQDqFTS",
        "outputId": "c6d4cf29-bc43-4908-b3f3-884dd063f1d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top Topics and Keywords for r/Singapore:\n",
            "Topic 1: china | gt | need | stupid | year | day | work | back | country | way\n",
            "Topic 2:  |  |  |  |  |  |  |  |  | \n",
            "Topic 3:  |  |  |  |  |  |  |  |  | \n",
            "Topic 4:  |  |  |  |  |  |  |  |  | \n",
            "Topic 5:  |  |  |  |  |  |  |  |  | \n",
            "Topic 6: well | dl | mark | hit |  |  |  |  |  | \n",
            "Topic 7: ahh | ohhh | ahhh | ahhhhh | stone | set | well |  |  | \n",
            "Topic 8:  |  |  |  |  |  |  |  |  | \n",
            "Topic 9: amp | page | canonical | summon | web | shared | privacy | uamputatorbot | bot | load\n",
            "Topic 10: riamatotalpieceofshit | riamapieceofshit | worthy |  |  |  |  |  |  | \n",
            "\n",
            "Top Topics and Keywords for r/SingaporeRaw:\n",
            "Topic 1: china | stupid | need | country | chinese | woman | sg | life | look | guy\n",
            "Topic 2:  |  |  |  |  |  |  |  |  | \n",
            "Topic 3: hdb | ssd | dweller | probaby | peks | stair | hdd | staircase | hdbs | lately\n",
            "Topic 4: difference | physiological | spino | notable | rchina | implying | shape | tail | three | indeed\n",
            "Topic 5: backfired | clownery | discipline | screenshots | screenshot | sending | saving | prevent | deleted | late\n",
            "Topic 6: find | searching | around | beating | route | finding | offer | help | literally | day\n",
            "Topic 7: badass | holy | looking |  |  |  |  |  |  | \n",
            "Topic 8: muscle | drooping | resting | facial | syndrome | cycle | difficult | leg | fast | face\n",
            "Topic 9: arcerms | location | warning | border | sort | close | asking | retarded | guy | \n",
            "Topic 10: roasting | hdd | chia | receive | crypto | specific | handle | google | luck | silly\n",
            "\n",
            "Top Topics and Keywords for r/SingaporeHappenings:\n",
            "Topic 1: driver | stupid | way | need | guy | look | back | someone | never | car\n",
            "Topic 2: holy |  |  |  |  |  |  |  |  | \n",
            "Topic 3: clown | around | goofy | mediocre | previously | chan | talent | perk | clowned | paporlumpars\n",
            "Topic 4: toilet | pee | hydraulic | wokeness | ponding | unaware | pool | london | demarcation | displeasure\n",
            "Topic 5: phone | version | android | boh | bother | mukhang | poly | temasek | rob | iphone\n",
            "Topic 6: spf | failed | resource | ownself | sgt | bodoh | gtwah | establishment | figuratively | gttoo\n",
            "Topic 7: need | destroy | basic | wayang | change | life |  |  |  | \n",
            "Topic 8: rush | run | ntuc | marathon | grown | fcking | mad | fool | grow | wow\n",
            "Topic 9: hdb | cow | lollll | fraud | cecas | stone | worship | stick | burn | insurance\n",
            "Topic 10: flat | inflation | cheaper | pricing | month | apply | article | resale | crash | divorced\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenAI API"
      ],
      "metadata": {
        "id": "WJfcFLXBqYCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv"
      ],
      "metadata": {
        "id": "mNMq7QfuvT-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "env_path = '/content/API.env'\n",
        "\n",
        "load_dotenv(env_path)\n"
      ],
      "metadata": {
        "id": "ZuGjI3BSvVW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Replace with your actual API key\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Dictionary to store representative themes for each subreddit\n",
        "representative_labels = {}\n",
        "\n",
        "# Example subreddit names\n",
        "subreddits = [\"r/Singapore\", \"r/SingaporeRaw\", \"r/SingaporeHappenings\"]\n",
        "\n",
        "# Iterate over each subreddit\n",
        "for subreddit in subreddits:\n",
        "    # Initialize a list to store themes for the subreddit\n",
        "    subreddit_themes = []\n",
        "\n",
        "    # List of lists of keywords for each topic in the current subreddit\n",
        "    keyword_groups = subreddit_topics.get(subreddit, [])\n",
        "\n",
        "    # Process each group of keywords\n",
        "    for topic_id, keywords in enumerate(keyword_groups):\n",
        "        # Join the keywords for each prompt\n",
        "        keywords_str = ', '.join([kw for kw in keywords if kw])  # Filter out empty strings\n",
        "\n",
        "        # Skip if keywords list is empty\n",
        "        if not keywords_str.strip():\n",
        "            continue\n",
        "\n",
        "        # Construct the prompt with Singaporean context\n",
        "        prompt = (\n",
        "            f\"Given these keywords related to a topic in a Singaporean context: {keywords_str}, \"\n",
        "            \"suggest a single concise word or phrase that best captures the main idea of this topic. The keywords are extracted from a dataset of toxic and harmful \"\n",
        "            \"comments, so the topics generated should likely be about topics that may cause resentment and irritation.\"\n",
        "        )\n",
        "\n",
        "        # Send the request to the ChatGPT API\n",
        "        response = openai.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=10,\n",
        "            temperature=0.7\n",
        "        )\n",
        "\n",
        "        # Extract and process the response as a single theme\n",
        "        theme = response.choices[0].message.content.strip().strip('\"')\n",
        "        subreddit_themes.append(theme)\n",
        "\n",
        "    # Store the themes for the current subreddit in `representative_labels`\n",
        "    representative_labels[subreddit] = subreddit_themes\n",
        "\n",
        "# Simplified and readable output\n",
        "for subreddit, themes in representative_labels.items():\n",
        "    print(f\"{subreddit}:\")\n",
        "    for theme in themes:\n",
        "        print(f\" - {theme}\")\n",
        "    print()  # Blank line between subreddits for readability\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EC_CXUlUqNwo",
        "outputId": "8add99c5-f947-4939-f3e9-8305aacb866f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "r/Singapore:\n",
            " - Foreign worker discrimination\n",
            " - Cyberbullying\n",
            " - Public behavior or etiquette\n",
            " - Web Privacy\n",
            " - Toxic Online Behavior\n",
            "\n",
            "r/SingaporeRaw:\n",
            " - Xenophobic attitudes towards Chinese people in Singapore\n",
            " - HDB staircase disputes\n",
            " - Racial differences in Singapore\n",
            " - Inappropriate Messaging\n",
            " - Lost Directions\n",
            " - Rebellious\n",
            " - Facial Paralysis\n",
            " - Border Control\n",
            " - Online scams\n",
            "\n",
            "r/SingaporeHappenings:\n",
            " - Reckless driving\n",
            " - religious insensitivity\n",
            " - Poor quality entertainment\n",
            " - Public hygiene and sanitation\n",
            " - Smartphone rivalry in Singapore.\n",
            " - Government incompetence\n",
            " - Inauthenticity\n",
            " - Singapore Marathon Chaos\n",
            " - HDB Scam\n",
            " - Housing Market Woes\n",
            "\n"
          ]
        }
      ]
    }
  ]
}