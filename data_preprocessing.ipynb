{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmolT_IYYaF9"
      },
      "outputs": [],
      "source": [
        "!pip install flair\n",
        "!pip install contractions\n",
        "!pip install simplemma"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bx7oj7zsvSrf",
        "outputId": "fc2e3e0a-3553-466e-ad23-a26bd144956c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QDa4YAZAdtG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import flair\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import contractions\n",
        "from simplemma import lemmatize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8teEGHgKBMF8"
      },
      "outputs": [],
      "source": [
        "# reading in data\n",
        "reddit_2021 = pd.read_csv('/content/drive/MyDrive/original/Reddit-Threads_2020-2021.csv', engine=\"python\")\n",
        "reddit_2223 = pd.read_csv('/content/drive/MyDrive/original/Reddit-Threads_2022-2023.csv', engine=\"python\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4vUTNw7H_kg"
      },
      "outputs": [],
      "source": [
        "# Remove entries with text '[deleted]' or '[removed]'\n",
        "reddit_2021 = reddit_2021[(reddit_2021['text'] != '[deleted]') & (reddit_2021['text'] != '[removed]')]\n",
        "reddit_2223 = reddit_2223[(reddit_2223['text'] != '[deleted]') & (reddit_2223['text'] != '[removed]')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQf96hC2rTVt"
      },
      "outputs": [],
      "source": [
        "# removing any special characters (eg. emoji)\n",
        "valid_characters_pattern = r'[^a-zA-Z0-9\\s.,!?\\'\"()\\\\-_$+=]'\n",
        "\n",
        "reddit_2021.loc[:, 'text'] = reddit_2021['text'].str.replace(valid_characters_pattern, '', regex=True)\n",
        "reddit_2223.loc[:, 'text'] = reddit_2223['text'].str.replace(valid_characters_pattern, '', regex=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hmp-usIVvug"
      },
      "outputs": [],
      "source": [
        "# removing empty text\n",
        "reddit_2021 = reddit_2021[reddit_2021['text'].notna() & (reddit_2021['text'].str.strip() != '')]\n",
        "reddit_2223 = reddit_2223[reddit_2223['text'].notna() & (reddit_2223['text'].str.strip() != '')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6MyJSyUtjPl"
      },
      "outputs": [],
      "source": [
        "# handling contractions\n",
        "reddit_2021['text'] = reddit_2021['text'].apply(lambda x: contractions.fix(x))\n",
        "reddit_2223['text'] = reddit_2223['text'].apply(lambda x: contractions.fix(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHfJUgpZR7jj"
      },
      "outputs": [],
      "source": [
        "# normalization - converting all text to lower case\n",
        "reddit_2021['text'] = reddit_2021['text'].str.lower()\n",
        "reddit_2223['text'] = reddit_2223['text'].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVen5z8UvNTC"
      },
      "outputs": [],
      "source": [
        "# function to remove single-letter words\n",
        "def remove_single_letter_words(text):\n",
        "    if isinstance(text, str):  # Ensure the text is a string\n",
        "        # Remove single-letter words using regex\n",
        "        text = re.sub(r'\\b\\w{1}\\b', '', text)\n",
        "        # Clean up extra spaces\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HItaTgJRvO1H"
      },
      "outputs": [],
      "source": [
        "# removing single letters\n",
        "reddit_2021['text'] = reddit_2021['text'].apply(remove_single_letter_words)\n",
        "reddit_2223['text'] = reddit_2223['text'].apply(remove_single_letter_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a1ES24vy05T"
      },
      "outputs": [],
      "source": [
        "# checking for empty text again\n",
        "reddit_2021 = reddit_2021[reddit_2021['text'].notna() & (reddit_2021['text'].str.strip() != '')]\n",
        "reddit_2223 = reddit_2223[reddit_2223['text'].notna() & (reddit_2223['text'].str.strip() != '')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "LNkjEvcdNQ3k"
      },
      "outputs": [],
      "source": [
        "# Creating 2 new columns: year and month\n",
        "reddit_2021.dropna(subset=['timestamp'], inplace=True)\n",
        "reddit_2021['timestamp'] = pd.to_datetime(reddit_2021['timestamp'])\n",
        "reddit_2021['year'] = reddit_2021['timestamp'].dt.year.astype(int)\n",
        "reddit_2021['month'] = reddit_2021['timestamp'].dt.month.astype(int)\n",
        "\n",
        "reddit_2223['timestamp'] = pd.to_datetime(reddit_2223['timestamp'])\n",
        "reddit_2223['year'] = reddit_2223['timestamp'].dt.year.astype(int)\n",
        "reddit_2223['month'] = reddit_2223['timestamp'].dt.month.astype(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "JrGX_auYOZOC"
      },
      "outputs": [],
      "source": [
        "# changing subreddit id to name\n",
        "reddit_2021['subreddit_name'] = reddit_2021['subreddit_id'].replace({\n",
        "    \"t5_2qh8c\": \"r/Singapore\",\n",
        "    \"t5_xnx04\": \"r/SingaporeRaw\"\n",
        "})\n",
        "\n",
        "reddit_2223['subreddit_name'] = reddit_2223['subreddit_id'].replace({\n",
        "    \"t5_2qh8c\": \"r/Singapore\",\n",
        "    \"t5_xnx04\": \"r/SingaporeRaw\",\n",
        "    \"t5_70s6ew\": \"r/SingaporeHappenings\"\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKZqJ_ONXpOl"
      },
      "outputs": [],
      "source": [
        "# taking a random subset of data - 20% of original data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, sample_2021 = train_test_split(reddit_2021, test_size=0.2, stratify=reddit_2021['subreddit_id'], random_state=42)\n",
        "\n",
        "train, sample_2223 = train_test_split(reddit_2223, test_size=0.2, stratify=reddit_2223['subreddit_id'], random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_t6OydX0lS7Z"
      },
      "outputs": [],
      "source": [
        "sample_2021.to_csv('working_2021.csv', index=False)\n",
        "sample_2223.to_csv('working_2223.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking spread of data**"
      ],
      "metadata": {
        "id": "90ZvqX12vb__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reddit2020 = reddit_2021[reddit_2021['year'] == 2020]\n",
        "reddit2021 = reddit_2021[reddit_2021['year'] == 2021]\n",
        "reddit2022 = reddit_2223[reddit_2223['year'] == 2022]\n",
        "reddit2023 = reddit_2223[reddit_2223['year'] == 2023]"
      ],
      "metadata": {
        "id": "Xbhe8Hj4v3Hy"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(reddit2020), len(reddit2021), len(reddit2022), len(reddit2023)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EUXFs7owLrC",
        "outputId": "1111c1f6-739c-4744-f660-a405214f2191"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1435876, 1513543, 1141853, 841456)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "totalfull = len(reddit2020) + len(reddit2021) + len(reddit2022) + len(reddit2023)"
      ],
      "metadata": {
        "id": "540httpBw4Yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(len(reddit2020)/totalfull * 100) , (len(reddit2021)/totalfull * 100), (len(reddit2022)/totalfull * 100) , (len(reddit2023)/totalfull * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtUiHefsxFHF",
        "outputId": "8acf08db-e378-4de2-dab3-1037bcfcef42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(29.10916636798137, 30.68369064744701, 23.148509303574006, 17.058633680997616)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample2020 = sample_2021[sample_2021['year'] == 2020]\n",
        "sample2021 = sample_2021[sample_2021['year'] == 2021]\n",
        "sample2022 = sample_2223[sample_2223['year'] == 2022]\n",
        "sample2023 = sample_2223[sample_2223['year'] == 2023]"
      ],
      "metadata": {
        "id": "qrSd12u8wlb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sample2020), len(sample2021), len(sample2022), len(sample2023)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCnnev_-wX6l",
        "outputId": "3eda8c53-8729-4440-b4b8-3ff8a69b3011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(259737, 270937, 207619, 159056)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total = len(sample2020) + len(sample2021) + len(sample2022) + len(sample2023)"
      ],
      "metadata": {
        "id": "BIhSvMtRxe_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(len(sample2020)/total * 100) , (len(sample2021)/total * 100), (len(sample2022)/total * 100) , (len(sample2023)/total * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odXcgbjDxmMQ",
        "outputId": "34ad8d6b-87a6-45c1-9cf9-86e74c4339ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28.944925552934254, 30.19304640669349, 23.136928887199964, 17.725099153172287)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sg2021 = reddit_2021[reddit_2021['subreddit_name'] == 'r/Singapore']\n",
        "sgraw2021 = reddit_2021[reddit_2021['subreddit_name'] == 'r/SingaporeRaw']\n",
        "\n",
        "sg2223 = reddit_2223[reddit_2223['subreddit_name'] == 'r/Singapore']\n",
        "sgraw2223 = reddit_2223[reddit_2223['subreddit_name'] == 'r/SingaporeRaw']\n",
        "sghpn2223 = reddit_2223[reddit_2223['subreddit_name'] == 'r/SingaporeHappenings']"
      ],
      "metadata": {
        "id": "bKcs5TNHZ4Cp"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sg2021), len(sgraw2021), len(sg2223), len(sgraw2223), len(sghpn2223)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxWXYJwRaQwP",
        "outputId": "932da77f-ff9c-45ca-f2e1-18f7ebac9a9a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2867222, 82197, 1577054, 358234, 48021)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samplesg2021 = sample_2021[sample_2021['subreddit_name'] == 'r/Singapore']\n",
        "samplesgraw2021 = sample_2021[sample_2021['subreddit_name'] == 'r/SingaporeRaw']\n",
        "\n",
        "samplesg2223 = sample_2223[sample_2223['subreddit_name'] == 'r/Singapore']\n",
        "samplesgraw2223 = sample_2223[sample_2223['subreddit_name'] == 'r/SingaporeRaw']\n",
        "samplesghpn2223 = sample_2223[sample_2223['subreddit_name'] == 'r/SingaporeHappenings']"
      ],
      "metadata": {
        "id": "5BnN_8gEbG1w"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(samplesg2021), len(samplesgraw2021), len(samplesg2223), len(samplesgraw2223), len(samplesghpn2223)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1mUEJGHbNdP",
        "outputId": "63081ad7-dc0f-404a-9872-f6d9743cad48"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(516086, 14588, 290475, 66853, 9347)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pe1t3yPgxZk9"
      },
      "source": [
        "**Reading in sample size data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xYzmI6SlYNb"
      },
      "outputs": [],
      "source": [
        "sample_2021 = pd.read_csv('working_2021.csv')\n",
        "sample_2223 = pd.read_csv('working_2223.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1akWzyNUclS"
      },
      "source": [
        "**SubWord Tokenization with BERT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGH79HwF69Zi"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('zanelim/singbert')\n",
        "model = BertModel.from_pretrained(\"zanelim/singbert\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7xlX0JhUg_d",
        "outputId": "12ad14fc-ed90-42ae-b630-70e172f68d3f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 530674/530674 [06:30<00:00, 1359.55it/s]\n"
          ]
        }
      ],
      "source": [
        "# Tokenization (subword with bert)\n",
        "subtokenization_2021 = []\n",
        "\n",
        "for text in tqdm(sample_2021['text']):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    subtokenization_2021.append(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9yrdoy1VNDP",
        "outputId": "974c0dd7-91b5-488d-9e5c-24df16764826"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 366675/366675 [04:12<00:00, 1450.76it/s]\n"
          ]
        }
      ],
      "source": [
        "# Tokenization (subword with bert)\n",
        "subtokenization_2223 = []\n",
        "\n",
        "for text in tqdm(sample_2223['text']):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    subtokenization_2223.append(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHok9U3JxHCg"
      },
      "source": [
        "**Lemmatization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tzF8awEXpCo",
        "outputId": "f0afc58a-2463-4b79-a554-3a5858f8ea91"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 530674/530674 [00:11<00:00, 45060.23it/s]\n"
          ]
        }
      ],
      "source": [
        "# Lemmatization\n",
        "sublemmatization_2021 = []\n",
        "\n",
        "for tokens in tqdm(subtokenization_2021):\n",
        "    lemmatized_tokens = [lemmatize(token, lang='en') for token in tokens]\n",
        "    sublemmatization_2021.append(lemmatized_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktyeUYSlYSqi",
        "outputId": "ce7ef9a8-1e80-49f0-92b3-828f2f8c27f4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 366675/366675 [00:09<00:00, 37848.35it/s]\n"
          ]
        }
      ],
      "source": [
        "# Lemmatization\n",
        "sublemmatization_2223 = []\n",
        "\n",
        "for tokens in tqdm(subtokenization_2223):\n",
        "    lemmatized_tokens = [lemmatize(token, lang='en') for token in tokens]\n",
        "    sublemmatization_2223.append(lemmatized_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HbPS1VOqx6e"
      },
      "source": [
        "**Token input ids**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahngx2LFq0ug",
        "outputId": "ba717018-b8e8-4931-e57f-b0616d6edfb9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 530674/530674 [07:20<00:00, 1205.00it/s]\n"
          ]
        }
      ],
      "source": [
        "input_ids_2021 = []\n",
        "\n",
        "for text in tqdm(sample_2021['text']):\n",
        "    tokens_id = tokenizer(text)['input_ids']\n",
        "    input_ids_2021.append(tokens_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DkGm-f-tI6a",
        "outputId": "a8b27763-86c5-4761-9b41-5922eaee8f2f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 366675/366675 [04:47<00:00, 1274.44it/s]\n"
          ]
        }
      ],
      "source": [
        "input_ids_2223 = []\n",
        "\n",
        "for text in tqdm(sample_2223['text']):\n",
        "    tokens_id = tokenizer(text)['input_ids']\n",
        "    input_ids_2223.append(tokens_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eapud57wIbxp"
      },
      "source": [
        "**Adding new columns**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0FSn6-YyBAD"
      },
      "outputs": [],
      "source": [
        "sample_2021['Tokenization'] = sublemmatization_2021\n",
        "sample_2223['Tokenization'] = sublemmatization_2223"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYPOqd_cydlG"
      },
      "outputs": [],
      "source": [
        "sample_2021['Input IDs'] = input_ids_2021\n",
        "sample_2223['Input IDs'] = input_ids_2223"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2aZuavay6wb"
      },
      "outputs": [],
      "source": [
        "sample_2021.to_csv('sample_2021.csv', index=False)\n",
        "sample_2223.to_csv('sample_2223.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_2021.to_csv('sample_2021wcleantext.csv', index=False)\n",
        "sample_2223.to_csv('sample_2223wcleantext.csv', index=False)"
      ],
      "metadata": {
        "id": "rj8N5e3Qqnoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5A_2CwiBc21"
      },
      "source": [
        "**Working with new data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJU9WvbY0GH-"
      },
      "outputs": [],
      "source": [
        "sample_2021 = pd.read_csv('sample_2021.csv')\n",
        "sample_2223 = pd.read_csv('sample_2223.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT_ya2jZ7i88"
      },
      "source": [
        "**Embedding- Singbert**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTSSIhUeZCt5"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWvfOz5tE3ii"
      },
      "outputs": [],
      "source": [
        "# Function to get embeddings\n",
        "def get_sentence_embedding(sentence):\n",
        "    # Tokenize and convert to tensors\n",
        "    inputs = tokenizer(sentence, max_length=512, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "    # Forward pass through the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Return the pooler_output as the sentence embedding\n",
        "    return outputs.pooler_output.squeeze().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3ur6E7zFRKT",
        "outputId": "30a6dfe0-aac2-48bf-c40c-789ead77267f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 530674/530674 [8:30:33<00:00, 17.32it/s]\n"
          ]
        }
      ],
      "source": [
        "singbert_2021 = []\n",
        "#import numpy as np\n",
        "\n",
        "for sentence in tqdm(sample_2021['text']):\n",
        "    output = get_sentence_embedding(sentence)\n",
        "    singbert_2021.append(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1ni3cc1XlsO",
        "outputId": "8a82ecd2-8430-469e-bdf0-9eb3b86afe41"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 366675/366675 [5:03:10<00:00, 20.16it/s]\n"
          ]
        }
      ],
      "source": [
        "singbert_2223 = []\n",
        "#import numpy as np\n",
        "\n",
        "for sentence in tqdm(sample_2223['text']):\n",
        "    output = get_sentence_embedding(sentence)\n",
        "    singbert_2223.append(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stopwords removal for hateful/toxic text**"
      ],
      "metadata": {
        "id": "8XN7ZauhSPK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "tjRtLZKoU9Gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reading in data obtained from roberta hate and roberta toxic model\n",
        "samplehatetoxic_2021 = pd.read_csv('/content/drive/MyDrive/Data Sample/samplehatetoxic_2021.csv')\n",
        "samplehatetoxic_2223 = pd.read_csv('/content/drive/MyDrive/Data Sample/samplehatetoxic_2223.csv')"
      ],
      "metadata": {
        "id": "YOpl8yeTSYc0"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reading in data obtained from bertoxic\n",
        "df = pd.read_csv('/content/drive/MyDrive/Data Sample/hatetoxic(lynn)_2021.csv')\n",
        "df2 = pd.read_csv('/content/drive/MyDrive/Data Sample/hatetoxic(lynn)_2223.csv')"
      ],
      "metadata": {
        "id": "K15hbDuR2Rz1"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adding toxic and hate columns from df to samplehatetoxic\n",
        "samplehatetoxic_2021['hate_label2'] = df['id_att']\n",
        "samplehatetoxic_2021['toxic_label2'] = df['toxicity']\n",
        "\n",
        "samplehatetoxic_2223['hate_label2'] = df2['id_att']\n",
        "samplehatetoxic_2223['toxic_label2'] = df2['toxicity']"
      ],
      "metadata": {
        "id": "oRr1_LHceg50"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samplehatetoxic_2021.to_csv('/content/drive/My Drive/hateandtoxic/combined_2021.csv', index=False)\n",
        "samplehatetoxic_2223.to_csv('/content/drive/My Drive/hateandtoxic/combined_2223.csv', index=False)"
      ],
      "metadata": {
        "id": "FJp5Aj6b1Y_t"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out rows where hate_label is 'HATE' or toxic_label is 'toxic'\n",
        "hatetoxic_2021 = samplehatetoxic_2021[\n",
        "    (samplehatetoxic_2021['hate_label'] == 'HATE') |\n",
        "    (samplehatetoxic_2021['toxic_label'] == 'toxic') |\n",
        "    (samplehatetoxic_2021['hate_label2'] == 1.) |\n",
        "    (samplehatetoxic_2021['toxic_label2'] == 1.)\n",
        "]\n",
        "\n",
        "hatetoxic_2223 = samplehatetoxic_2223[\n",
        "    (samplehatetoxic_2223['hate_label'] == 'HATE') |\n",
        "    (samplehatetoxic_2223['toxic_label'] == 'toxic') |\n",
        "    (samplehatetoxic_2223['hate_label2'] == 1.) |\n",
        "    (samplehatetoxic_2223['toxic_label2'] == 1.)\n",
        "]"
      ],
      "metadata": {
        "id": "8Q0O8zNLSxww"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the file paths in your Google Drive\n",
        "hatetoxic_2021_path = '/content/drive/My Drive/hateandtoxic/hatetoxic_2021.csv'\n",
        "hatetoxic_2223_path = '/content/drive/My Drive/hateandtoxic/hatetoxic_2223.csv'\n",
        "\n",
        "# Save the DataFrames as CSV files\n",
        "hatetoxic_2021.to_csv(hatetoxic_2021_path, index=False)\n",
        "hatetoxic_2223.to_csv(hatetoxic_2223_path, index=False)"
      ],
      "metadata": {
        "id": "-kVWFt1MFOU5"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hatetoxic_2021 = pd.read_csv('/content/drive/MyDrive/hateandtoxic/hatetoxic_2021.csv')\n",
        "hatetoxic_2223 = pd.read_csv('/content/drive/MyDrive/hateandtoxic/hatetoxic_2223.csv')"
      ],
      "metadata": {
        "id": "Re-N1cvBFmjn"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "9887D3HMVINP"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removal of commonly used words in Singapore\n",
        "stop_words.update(['lah', 'lor', 'leh', 'liao', 'meh', 'mah', 'ah', 'hor', 'wah', 'lah','la', 'hor', 'sian', 'see', 'yeah', 'ya', 'yah', 'le','ba', 'bah', 'haha', 'bro','want','wants'\n",
        "                    'already', 'also', 'one', 'can', 'cannot', 'got', 'like', 'really', 'lol', 'lmao', 'yes', 'no', 'eh' ,'ah', 'omg', 'go', 'get', 'must', 'man','one', 'know', 'need'\n",
        "                    'sia', 'walao', 'siao', 'alamak', 'confirm', 'makan','aiyah', 'aiyo', 'aiyah','sure','even','probably','think', 'ok', 'okay', 'tbh','make', 'n',\n",
        "                    'still', 'maybe','said','you know', 'i mean', 'like that', 'do not know', 'not sure', 'of course', 'how come','always','alway','say', 'damn','give', 'going', 'take', 'took',\n",
        "                   'would','should','could','thing', 'right','oh','ah','as','sia'])"
      ],
      "metadata": {
        "id": "SkmOpBc-QWbm"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    # Tokenization\n",
        "    words = word_tokenize(text.lower())\n",
        "    # Remove punctuation and non-alphabetic tokens\n",
        "    words = [word for word in words if word.isalpha()]\n",
        "    # Stopword removal and lemmatization\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "\n",
        "    return ' '.join(words)"
      ],
      "metadata": {
        "id": "3-_kTYsHY3SM"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing stopwords and punctuation\n",
        "hatetoxic_preprocessed_2021 = [preprocess(text) for text in hatetoxic_2021['text']]\n",
        "hatetoxic_preprocessed_2223 = [preprocess(text) for text in hatetoxic_2223['text']]"
      ],
      "metadata": {
        "id": "att9vU8WY43r"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all entries to string\n",
        "hatetoxic_preprocessed_2021 = [str(text) for text in hatetoxic_preprocessed_2021]\n",
        "hatetoxic_preprocessed_2223 = [str(text) for text in hatetoxic_preprocessed_2223]"
      ],
      "metadata": {
        "id": "Y-VneYUh30W4"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hatetoxic_2021.loc[:, 'text without punctuation and stopword'] = hatetoxic_preprocessed_2021\n",
        "hatetoxic_2223.loc[:, 'text without punctuation and stopword'] = hatetoxic_preprocessed_2223"
      ],
      "metadata": {
        "id": "cMuo9Ay5ZRAS"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking and removing empty rows in 'text without punctuation and stopword' column after cleaning\n",
        "hatetoxic_2021 = hatetoxic_2021[\n",
        "    (hatetoxic_2021['text without punctuation and stopword'].str.strip() != '') &  # Condition to check for non-empty strings\n",
        "    (hatetoxic_2021['text without punctuation and stopword'].notna())            # Condition to check for non-NaN values\n",
        "]\n",
        "\n",
        "\n",
        "hatetoxic_2223 = hatetoxic_2223[\n",
        "    (hatetoxic_2223['text without punctuation and stopword'].str.strip() != '') &  # Condition to check for non-empty strings\n",
        "    (hatetoxic_2223['text without punctuation and stopword'].notna())            # Condition to check for non-NaN values\n",
        "]"
      ],
      "metadata": {
        "id": "TBNqjjuz4Ymb"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adding files with text wo punc and stopwords column\n",
        "file_path = '/content/drive/MyDrive/Data Sample/hatetoxic_2021.csv'\n",
        "hatetoxic_2021.to_csv(file_path, index=False)"
      ],
      "metadata": {
        "id": "D9Q9uEtPPEZK"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/Data Sample/hatetoxic_2223.csv'\n",
        "hatetoxic_2223.to_csv(file_path, index=False)"
      ],
      "metadata": {
        "id": "OQQ7RYNZdyJm"
      },
      "execution_count": 75,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}