{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmolT_IYYaF9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb289ada-f110-4261-87f4-9c421b5b155f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting simplemma\n",
            "  Downloading simplemma-1.1.1-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading simplemma-1.1.1-py3-none-any.whl (67.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.2/67.2 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: simplemma\n",
            "Successfully installed simplemma-1.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install flair\n",
        "!pip install contractions\n",
        "!pip install simplemma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QDa4YAZAdtG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import flair\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import contractions\n",
        "from simplemma import lemmatize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8teEGHgKBMF8"
      },
      "outputs": [],
      "source": [
        "# reading in data\n",
        "reddit_2021 = pd.read_csv(\"Reddit-Threads_2020-2021.csv\", engine=\"python\")\n",
        "reddit_2223 = pd.read_csv(\"Reddit-Threads_2022-2023.csv\", engine=\"python\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4vUTNw7H_kg"
      },
      "outputs": [],
      "source": [
        "# Remove entries with text '[deleted]' or '[removed]'\n",
        "reddit_2021 = reddit_2021[(reddit_2021['text'] != '[deleted]') & (reddit_2021['text'] != '[removed]')]\n",
        "reddit_2223 = reddit_2223[(reddit_2223['text'] != '[deleted]') & (reddit_2223['text'] != '[removed]')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQf96hC2rTVt"
      },
      "outputs": [],
      "source": [
        "# removing any special characters (eg. emoji)\n",
        "valid_characters_pattern = r'[^a-zA-Z0-9\\s.,!?\\'\"()\\\\-_$+=]'\n",
        "\n",
        "reddit_2021.loc[:, 'text'] = reddit_2021['text'].str.replace(valid_characters_pattern, '', regex=True)\n",
        "reddit_2223.loc[:, 'text'] = reddit_2223['text'].str.replace(valid_characters_pattern, '', regex=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hmp-usIVvug"
      },
      "outputs": [],
      "source": [
        "# removing empty text\n",
        "reddit_2021 = reddit_2021[reddit_2021['text'].notna() & (reddit_2021['text'].str.strip() != '')]\n",
        "reddit_2223 = reddit_2223[reddit_2223['text'].notna() & (reddit_2223['text'].str.strip() != '')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6MyJSyUtjPl"
      },
      "outputs": [],
      "source": [
        "# handling contractions\n",
        "reddit_2021['text'] = reddit_2021['text'].apply(lambda x: contractions.fix(x))\n",
        "reddit_2223['text'] = reddit_2223['text'].apply(lambda x: contractions.fix(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHfJUgpZR7jj"
      },
      "outputs": [],
      "source": [
        "# normalization - converting all text to lower case\n",
        "reddit_2021['text'] = reddit_2021['text'].str.lower()\n",
        "reddit_2223['text'] = reddit_2223['text'].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVen5z8UvNTC"
      },
      "outputs": [],
      "source": [
        "# function to remove single-letter words\n",
        "def remove_single_letter_words(text):\n",
        "    if isinstance(text, str):  # Ensure the text is a string\n",
        "        # Remove single-letter words using regex\n",
        "        text = re.sub(r'\\b\\w{1}\\b', '', text)\n",
        "        # Clean up extra spaces\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HItaTgJRvO1H"
      },
      "outputs": [],
      "source": [
        "# removing single letters\n",
        "reddit_2021['text'] = reddit_2021['text'].apply(remove_single_letter_words)\n",
        "reddit_2223['text'] = reddit_2223['text'].apply(remove_single_letter_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a1ES24vy05T"
      },
      "outputs": [],
      "source": [
        "# checking for empty text again\n",
        "reddit_2021 = reddit_2021[reddit_2021['text'].notna() & (reddit_2021['text'].str.strip() != '')]\n",
        "reddit_2223 = reddit_2223[reddit_2223['text'].notna() & (reddit_2223['text'].str.strip() != '')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNkjEvcdNQ3k"
      },
      "outputs": [],
      "source": [
        "# Creating 2 new columns: year and month\n",
        "reddit_2021.dropna(subset=['timestamp'], inplace=True)\n",
        "reddit_2021['timestamp'] = pd.to_datetime(reddit_2021['timestamp'])\n",
        "reddit_2021['year'] = reddit_2021['timestamp'].dt.year.astype(int)\n",
        "reddit_2021['month'] = reddit_2021['timestamp'].dt.month.astype(int)\n",
        "\n",
        "reddit_2223['timestamp'] = pd.to_datetime(reddit_2223['timestamp'])\n",
        "reddit_2223['year'] = reddit_2223['timestamp'].dt.year.astype(int)\n",
        "reddit_2223['month'] = reddit_2223['timestamp'].dt.month.astype(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrGX_auYOZOC"
      },
      "outputs": [],
      "source": [
        "# changing subreddit id to name\n",
        "reddit_2021['subreddit_name'] = reddit_2021['subreddit_id'].replace({\n",
        "    \"t5_2qh8c\": \"r/Singapore\",\n",
        "    \"t5_xnx04\": \"r/SingaporeRaw\"\n",
        "})\n",
        "\n",
        "reddit_2223['subreddit_name'] = reddit_2223['subreddit_id'].replace({\n",
        "    \"t5_2qh8c\": \"r/Singapore\",\n",
        "    \"t5_xnx04\": \"r/SingaporeRaw\",\n",
        "    \"t5_70s6ew\": \"r/SingaporeHappenings\"\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKZqJ_ONXpOl"
      },
      "outputs": [],
      "source": [
        "# taking a random subset of data - 20% of original data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, sample_2021 = train_test_split(reddit_2021, test_size=0.2, stratify=reddit_2021['subreddit_id'], random_state=42)\n",
        "\n",
        "train, sample_2223 = train_test_split(reddit_2223, test_size=0.2, stratify=reddit_2223['subreddit_id'], random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_t6OydX0lS7Z"
      },
      "outputs": [],
      "source": [
        "sample_2021.to_csv('working_2021.csv', index=False)\n",
        "sample_2223.to_csv('working_2223.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pe1t3yPgxZk9"
      },
      "source": [
        "**Reading in sample size data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xYzmI6SlYNb"
      },
      "outputs": [],
      "source": [
        "sample_2021 = pd.read_csv('working_2021.csv')\n",
        "sample_2223 = pd.read_csv('working_2223.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1akWzyNUclS"
      },
      "source": [
        "**SubWord Tokenization with BERT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGH79HwF69Zi"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('zanelim/singbert')\n",
        "model = BertModel.from_pretrained(\"zanelim/singbert\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7xlX0JhUg_d",
        "outputId": "12ad14fc-ed90-42ae-b630-70e172f68d3f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 530674/530674 [06:30<00:00, 1359.55it/s]\n"
          ]
        }
      ],
      "source": [
        "# Tokenization (subword with bert)\n",
        "subtokenization_2021 = []\n",
        "\n",
        "for text in tqdm(sample_2021['text']):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    subtokenization_2021.append(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9yrdoy1VNDP",
        "outputId": "974c0dd7-91b5-488d-9e5c-24df16764826"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 366675/366675 [04:12<00:00, 1450.76it/s]\n"
          ]
        }
      ],
      "source": [
        "# Tokenization (subword with bert)\n",
        "subtokenization_2223 = []\n",
        "\n",
        "for text in tqdm(sample_2223['text']):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    subtokenization_2223.append(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHok9U3JxHCg"
      },
      "source": [
        "**Lemmatization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tzF8awEXpCo",
        "outputId": "f0afc58a-2463-4b79-a554-3a5858f8ea91"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 530674/530674 [00:11<00:00, 45060.23it/s]\n"
          ]
        }
      ],
      "source": [
        "# Lemmatization\n",
        "sublemmatization_2021 = []\n",
        "\n",
        "for tokens in tqdm(subtokenization_2021):\n",
        "    lemmatized_tokens = [lemmatize(token, lang='en') for token in tokens]\n",
        "    sublemmatization_2021.append(lemmatized_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktyeUYSlYSqi",
        "outputId": "ce7ef9a8-1e80-49f0-92b3-828f2f8c27f4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 366675/366675 [00:09<00:00, 37848.35it/s]\n"
          ]
        }
      ],
      "source": [
        "# Lemmatization\n",
        "sublemmatization_2223 = []\n",
        "\n",
        "for tokens in tqdm(subtokenization_2223):\n",
        "    lemmatized_tokens = [lemmatize(token, lang='en') for token in tokens]\n",
        "    sublemmatization_2223.append(lemmatized_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HbPS1VOqx6e"
      },
      "source": [
        "**Token input ids**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahngx2LFq0ug",
        "outputId": "ba717018-b8e8-4931-e57f-b0616d6edfb9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 530674/530674 [07:20<00:00, 1205.00it/s]\n"
          ]
        }
      ],
      "source": [
        "input_ids_2021 = []\n",
        "\n",
        "for text in tqdm(sample_2021['text']):\n",
        "    tokens_id = tokenizer(text)['input_ids']\n",
        "    input_ids_2021.append(tokens_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DkGm-f-tI6a",
        "outputId": "a8b27763-86c5-4761-9b41-5922eaee8f2f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 366675/366675 [04:47<00:00, 1274.44it/s]\n"
          ]
        }
      ],
      "source": [
        "input_ids_2223 = []\n",
        "\n",
        "for text in tqdm(sample_2223['text']):\n",
        "    tokens_id = tokenizer(text)['input_ids']\n",
        "    input_ids_2223.append(tokens_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNl-EvUZBoLM"
      },
      "source": [
        "**Removing stop words and punctuation (useful for topic modeling)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHCp_AXSE6lN"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-SrgN_8Eus4"
      },
      "outputs": [],
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "#removing commonly used singlish terms that does not hold sentiment value\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.update(['also', 'mr', 'mrs','miss','mdm','ya', 'yah', 'la', 'lah','lor','leh','liao','hor','mah','meh','sia','lol','lmao','like','yes','no'])\n",
        "\n",
        "def preprocess(text):\n",
        "    # Tokenization\n",
        "    words = word_tokenize(text.lower())\n",
        "    # Remove punctuation and non-alphabetic tokens\n",
        "    words = [word for word in words if word.isalpha()]\n",
        "    # Stopword removal and lemmatization\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "\n",
        "    return ' '.join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sq8Sn7E2E7-p"
      },
      "outputs": [],
      "source": [
        "# Removing stopwords and punctuation\n",
        "texts_preprocessed_2021 = [preprocess(text) for text in sample_2021['text']]\n",
        "text_preprocessed_2223 = [preprocess(text) for text in sample_2223['text']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eapud57wIbxp"
      },
      "source": [
        "**Adding new columns**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0FSn6-YyBAD"
      },
      "outputs": [],
      "source": [
        "sample_2021['Tokenization'] = sublemmatization_2021\n",
        "sample_2223['Tokenization'] = sublemmatization_2223"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYPOqd_cydlG"
      },
      "outputs": [],
      "source": [
        "sample_2021['Input IDs'] = input_ids_2021\n",
        "sample_2223['Input IDs'] = input_ids_2223"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2aZuavay6wb"
      },
      "outputs": [],
      "source": [
        "sample_2021.to_csv('sample_2021.csv', index=False)\n",
        "sample_2223.to_csv('sample_2223.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XC_L777MG64Z"
      },
      "outputs": [],
      "source": [
        "sample_2021['text without punctuation and stopword'] = texts_preprocessed_2021\n",
        "sample_2223['text without punctuation and stopword'] = text_preprocessed_2223"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_2021.to_csv('sample_2021wcleantext.csv', index=False)\n",
        "sample_2223.to_csv('sample_2223wcleantext.csv', index=False)"
      ],
      "metadata": {
        "id": "rj8N5e3Qqnoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5A_2CwiBc21"
      },
      "source": [
        "**Working with new data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJU9WvbY0GH-"
      },
      "outputs": [],
      "source": [
        "sample_2021 = pd.read_csv('sample_2021.csv')\n",
        "sample_2223 = pd.read_csv('sample_2223.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT_ya2jZ7i88"
      },
      "source": [
        "**Embedding- Singbert**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTSSIhUeZCt5"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWvfOz5tE3ii"
      },
      "outputs": [],
      "source": [
        "# Function to get embeddings\n",
        "def get_sentence_embedding(sentence):\n",
        "    # Tokenize and convert to tensors\n",
        "    inputs = tokenizer(sentence, max_length=512, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "    # Forward pass through the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Return the pooler_output as the sentence embedding\n",
        "    return outputs.pooler_output.squeeze().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3ur6E7zFRKT",
        "outputId": "30a6dfe0-aac2-48bf-c40c-789ead77267f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 530674/530674 [8:30:33<00:00, 17.32it/s]\n"
          ]
        }
      ],
      "source": [
        "singbert_2021 = []\n",
        "#import numpy as np\n",
        "\n",
        "for sentence in tqdm(sample_2021['text']):\n",
        "    output = get_sentence_embedding(sentence)\n",
        "    singbert_2021.append(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1ni3cc1XlsO",
        "outputId": "8a82ecd2-8430-469e-bdf0-9eb3b86afe41"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 366675/366675 [5:03:10<00:00, 20.16it/s]\n"
          ]
        }
      ],
      "source": [
        "singbert_2223 = []\n",
        "#import numpy as np\n",
        "\n",
        "for sentence in tqdm(sample_2223['text']):\n",
        "    output = get_sentence_embedding(sentence)\n",
        "    singbert_2223.append(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8FVsrnmI3tg"
      },
      "source": [
        "**Embedding - Singbert: for text without stopwords and punctuation (useful for topic modeling)**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_2021wcleantext = pd.read_csv('sample_2021wcleantext.csv')\n",
        "sample_2223wcleantext = pd.read_csv('sample_2223wcleantext.csv')"
      ],
      "metadata": {
        "id": "Ej-xEYBysMyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTn1-byp1aO5"
      },
      "outputs": [],
      "source": [
        "# checking and removing empty rows in 'text without punctuation and stopword' column after cleaning\n",
        "sample_2021wcleantext = sample_2021wcleantext[\n",
        "    (sample_2021wcleantext['text without punctuation and stopword'].str.strip() != '') &  # Condition to check for non-empty strings\n",
        "    (sample_2021wcleantext['text without punctuation and stopword'].notna())            # Condition to check for non-NaN values\n",
        "]\n",
        "\n",
        "\n",
        "sample_2223wcleantext = sample_2223wcleantext[\n",
        "    (sample_2223wcleantext['text without punctuation and stopword'].str.strip() != '') &  # Condition to check for non-empty strings\n",
        "    (sample_2223wcleantext['text without punctuation and stopword'].notna())            # Condition to check for non-NaN values\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_2021wcleantext.to_csv('sample(punc_stopwords_removed)_2021.csv', index=False)\n",
        "sample_2223wcleantext.to_csv('sample(punc_stopwords_removed)_2223.csv', index=False)"
      ],
      "metadata": {
        "id": "yaMDS1478-pN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_cleaned_2021 = pd.read_csv('sample(punc_stopwords_removed)_2021.csv')\n",
        "sample_cleaned_2223 = pd.read_csv('sample(punc_stopwords_removed)_2223.csv')"
      ],
      "metadata": {
        "id": "YvSaAthWogtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeLidGF0I-Fg",
        "outputId": "40865836-942c-422e-8f41-9966e234f09e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 524379/524379 [6:03:41<00:00, 24.03it/s]\n"
          ]
        }
      ],
      "source": [
        "subsingbert_2021 = []\n",
        "#import numpy as np\n",
        "\n",
        "for sentence in tqdm(sample_cleaned_2021['text without punctuation and stopword']):\n",
        "    output = get_sentence_embedding(sentence)\n",
        "    subsingbert_2021.append(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5t0p3RYJUPk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3543b1c4-5d8c-4cc6-b819-8aa76876a316"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 362624/362624 [4:29:11<00:00, 22.45it/s]\n"
          ]
        }
      ],
      "source": [
        "subsingbert_2223 = []\n",
        "#import numpy as np\n",
        "\n",
        "for sentence in tqdm(sample_cleaned_2223['text without punctuation and stopword']):\n",
        "    output = get_sentence_embedding(sentence)\n",
        "    subsingbert_2223.append(output)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}