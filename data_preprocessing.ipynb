{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmolT_IYYaF9"
      },
      "outputs": [],
      "source": [
        "!pip install flair\n",
        "!pip install contractions\n",
        "!pip install simplemma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QDa4YAZAdtG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import flair\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import contractions\n",
        "from simplemma import lemmatize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8teEGHgKBMF8"
      },
      "outputs": [],
      "source": [
        "# reading in data\n",
        "reddit_2021 = pd.read_csv(\"Reddit-Threads_2020-2021.csv\", engine=\"python\")\n",
        "reddit_2223 = pd.read_csv(\"Reddit-Threads_2022-2023.csv\", engine=\"python\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4vUTNw7H_kg"
      },
      "outputs": [],
      "source": [
        "# Remove entries with text '[deleted]' or '[removed]'\n",
        "reddit_2021 = reddit_2021[(reddit_2021['text'] != '[deleted]') & (reddit_2021['text'] != '[removed]')]\n",
        "reddit_2223 = reddit_2223[(reddit_2223['text'] != '[deleted]') & (reddit_2223['text'] != '[removed]')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQf96hC2rTVt"
      },
      "outputs": [],
      "source": [
        "# removing any special characters (eg. emoji)\n",
        "valid_characters_pattern = r'[^a-zA-Z0-9\\s.,!?\\'\"()\\\\-_$+=]'\n",
        "\n",
        "reddit_2021.loc[:, 'text'] = reddit_2021['text'].str.replace(valid_characters_pattern, '', regex=True)\n",
        "reddit_2223.loc[:, 'text'] = reddit_2223['text'].str.replace(valid_characters_pattern, '', regex=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hmp-usIVvug"
      },
      "outputs": [],
      "source": [
        "# removing empty text\n",
        "reddit_2021 = reddit_2021[reddit_2021['text'].notna() & (reddit_2021['text'].str.strip() != '')]\n",
        "reddit_2223 = reddit_2223[reddit_2223['text'].notna() & (reddit_2223['text'].str.strip() != '')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6MyJSyUtjPl"
      },
      "outputs": [],
      "source": [
        "# handling contractions\n",
        "reddit_2021['text'] = reddit_2021['text'].apply(lambda x: contractions.fix(x))\n",
        "reddit_2223['text'] = reddit_2223['text'].apply(lambda x: contractions.fix(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHfJUgpZR7jj"
      },
      "outputs": [],
      "source": [
        "# normalization - converting all text to lower case\n",
        "reddit_2021['text'] = reddit_2021['text'].str.lower()\n",
        "reddit_2223['text'] = reddit_2223['text'].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVen5z8UvNTC"
      },
      "outputs": [],
      "source": [
        "# function to remove single-letter words\n",
        "def remove_single_letter_words(text):\n",
        "    if isinstance(text, str):  # Ensure the text is a string\n",
        "        # Remove single-letter words using regex\n",
        "        text = re.sub(r'\\b\\w{1}\\b', '', text)\n",
        "        # Clean up extra spaces\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HItaTgJRvO1H"
      },
      "outputs": [],
      "source": [
        "# removing single letters\n",
        "reddit_2021['text'] = reddit_2021['text'].apply(remove_single_letter_words)\n",
        "reddit_2223['text'] = reddit_2223['text'].apply(remove_single_letter_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a1ES24vy05T"
      },
      "outputs": [],
      "source": [
        "# checking for empty text again\n",
        "reddit_2021 = reddit_2021[reddit_2021['text'].notna() & (reddit_2021['text'].str.strip() != '')]\n",
        "reddit_2223 = reddit_2223[reddit_2223['text'].notna() & (reddit_2223['text'].str.strip() != '')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNkjEvcdNQ3k"
      },
      "outputs": [],
      "source": [
        "# Creating 2 new columns: year and month\n",
        "reddit_2021.dropna(subset=['timestamp'], inplace=True)\n",
        "reddit_2021['timestamp'] = pd.to_datetime(reddit_2021['timestamp'])\n",
        "reddit_2021['year'] = reddit_2021['timestamp'].dt.year.astype(int)\n",
        "reddit_2021['month'] = reddit_2021['timestamp'].dt.month.astype(int)\n",
        "\n",
        "reddit_2223['timestamp'] = pd.to_datetime(reddit_2223['timestamp'])\n",
        "reddit_2223['year'] = reddit_2223['timestamp'].dt.year.astype(int)\n",
        "reddit_2223['month'] = reddit_2223['timestamp'].dt.month.astype(int)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrGX_auYOZOC"
      },
      "outputs": [],
      "source": [
        "# changing subreddit id to name\n",
        "reddit_2021['subreddit_name'] = reddit_2021['subreddit_id'].replace({\n",
        "    \"t5_2qh8c\": \"r/Singapore\",\n",
        "    \"t5_xnx04\": \"r/SingaporeRaw\"\n",
        "})\n",
        "\n",
        "reddit_2223['subreddit_name'] = reddit_2223['subreddit_id'].replace({\n",
        "    \"t5_2qh8c\": \"r/Singapore\",\n",
        "    \"t5_xnx04\": \"r/SingaporeRaw\",\n",
        "    \"t5_70s6ew\": \"r/SingaporeHappenings\"\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKZqJ_ONXpOl"
      },
      "outputs": [],
      "source": [
        "# taking a random subset of data - 20% of original data\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, sample_2021 = train_test_split(reddit_2021, test_size=0.2, stratify=reddit_2021['subreddit_id'], random_state=42)\n",
        "\n",
        "train, sample_2223 = train_test_split(reddit_2223, test_size=0.2, stratify=reddit_2223['subreddit_id'], random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_t6OydX0lS7Z"
      },
      "outputs": [],
      "source": [
        "sample_2021.to_csv('working_2021.csv', index=False)\n",
        "sample_2223.to_csv('working_2223.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pe1t3yPgxZk9"
      },
      "source": [
        "**Reading in sample size data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xYzmI6SlYNb"
      },
      "outputs": [],
      "source": [
        "sample_2021 = pd.read_csv('working_2021.csv')\n",
        "sample_2223 = pd.read_csv('working_2223.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1akWzyNUclS"
      },
      "source": [
        "**SubWord Tokenization with BERT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGH79HwF69Zi"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('zanelim/singbert')\n",
        "model = BertModel.from_pretrained(\"zanelim/singbert\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7xlX0JhUg_d",
        "outputId": "12ad14fc-ed90-42ae-b630-70e172f68d3f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 530674/530674 [06:30<00:00, 1359.55it/s]\n"
          ]
        }
      ],
      "source": [
        "# Tokenization (subword with bert)\n",
        "subtokenization_2021 = []\n",
        "\n",
        "for text in tqdm(sample_2021['text']):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    subtokenization_2021.append(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9yrdoy1VNDP",
        "outputId": "974c0dd7-91b5-488d-9e5c-24df16764826"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 366675/366675 [04:12<00:00, 1450.76it/s]\n"
          ]
        }
      ],
      "source": [
        "# Tokenization (subword with bert)\n",
        "subtokenization_2223 = []\n",
        "\n",
        "for text in tqdm(sample_2223['text']):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    subtokenization_2223.append(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHok9U3JxHCg"
      },
      "source": [
        "**Lemmatization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tzF8awEXpCo",
        "outputId": "f0afc58a-2463-4b79-a554-3a5858f8ea91"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 530674/530674 [00:11<00:00, 45060.23it/s]\n"
          ]
        }
      ],
      "source": [
        "# Lemmatization\n",
        "sublemmatization_2021 = []\n",
        "\n",
        "for tokens in tqdm(subtokenization_2021):\n",
        "    lemmatized_tokens = [lemmatize(token, lang='en') for token in tokens]\n",
        "    sublemmatization_2021.append(lemmatized_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktyeUYSlYSqi",
        "outputId": "ce7ef9a8-1e80-49f0-92b3-828f2f8c27f4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 366675/366675 [00:09<00:00, 37848.35it/s]\n"
          ]
        }
      ],
      "source": [
        "# Lemmatization\n",
        "sublemmatization_2223 = []\n",
        "\n",
        "for tokens in tqdm(subtokenization_2223):\n",
        "    lemmatized_tokens = [lemmatize(token, lang='en') for token in tokens]\n",
        "    sublemmatization_2223.append(lemmatized_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HbPS1VOqx6e"
      },
      "source": [
        "**Token input ids**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahngx2LFq0ug",
        "outputId": "ba717018-b8e8-4931-e57f-b0616d6edfb9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 530674/530674 [07:20<00:00, 1205.00it/s]\n"
          ]
        }
      ],
      "source": [
        "input_ids_2021 = []\n",
        "\n",
        "for text in tqdm(sample_2021['text']):\n",
        "    tokens_id = tokenizer(text)['input_ids']\n",
        "    input_ids_2021.append(tokens_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DkGm-f-tI6a",
        "outputId": "a8b27763-86c5-4761-9b41-5922eaee8f2f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 366675/366675 [04:47<00:00, 1274.44it/s]\n"
          ]
        }
      ],
      "source": [
        "input_ids_2223 = []\n",
        "\n",
        "for text in tqdm(sample_2223['text']):\n",
        "    tokens_id = tokenizer(text)['input_ids']\n",
        "    input_ids_2223.append(tokens_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eapud57wIbxp"
      },
      "source": [
        "**Adding new columns**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0FSn6-YyBAD"
      },
      "outputs": [],
      "source": [
        "sample_2021['Tokenization'] = sublemmatization_2021\n",
        "sample_2223['Tokenization'] = sublemmatization_2223"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYPOqd_cydlG"
      },
      "outputs": [],
      "source": [
        "sample_2021['Input IDs'] = input_ids_2021\n",
        "sample_2223['Input IDs'] = input_ids_2223"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2aZuavay6wb"
      },
      "outputs": [],
      "source": [
        "sample_2021.to_csv('sample_2021.csv', index=False)\n",
        "sample_2223.to_csv('sample_2223.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XC_L777MG64Z"
      },
      "outputs": [],
      "source": [
        "sample_2021['text without punctuation and stopword'] = texts_preprocessed_2021\n",
        "sample_2223['text without punctuation and stopword'] = text_preprocessed_2223"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_2021.to_csv('sample_2021wcleantext.csv', index=False)\n",
        "sample_2223.to_csv('sample_2223wcleantext.csv', index=False)"
      ],
      "metadata": {
        "id": "rj8N5e3Qqnoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5A_2CwiBc21"
      },
      "source": [
        "**Working with new data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJU9WvbY0GH-"
      },
      "outputs": [],
      "source": [
        "sample_2021 = pd.read_csv('sample_2021.csv')\n",
        "sample_2223 = pd.read_csv('sample_2223.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT_ya2jZ7i88"
      },
      "source": [
        "**Embedding- Singbert**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTSSIhUeZCt5"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWvfOz5tE3ii"
      },
      "outputs": [],
      "source": [
        "# Function to get embeddings\n",
        "def get_sentence_embedding(sentence):\n",
        "    # Tokenize and convert to tensors\n",
        "    inputs = tokenizer(sentence, max_length=512, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "    # Forward pass through the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Return the pooler_output as the sentence embedding\n",
        "    return outputs.pooler_output.squeeze().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3ur6E7zFRKT",
        "outputId": "30a6dfe0-aac2-48bf-c40c-789ead77267f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 530674/530674 [8:30:33<00:00, 17.32it/s]\n"
          ]
        }
      ],
      "source": [
        "singbert_2021 = []\n",
        "#import numpy as np\n",
        "\n",
        "for sentence in tqdm(sample_2021['text']):\n",
        "    output = get_sentence_embedding(sentence)\n",
        "    singbert_2021.append(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1ni3cc1XlsO",
        "outputId": "8a82ecd2-8430-469e-bdf0-9eb3b86afe41"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 366675/366675 [5:03:10<00:00, 20.16it/s]\n"
          ]
        }
      ],
      "source": [
        "singbert_2223 = []\n",
        "#import numpy as np\n",
        "\n",
        "for sentence in tqdm(sample_2223['text']):\n",
        "    output = get_sentence_embedding(sentence)\n",
        "    singbert_2223.append(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stopwords removal for hateful/toxic text**"
      ],
      "metadata": {
        "id": "8XN7ZauhSPK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "bWT0ga9LSpqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "tjRtLZKoU9Gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "tWEQafvTSS-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samplehatetoxic_2021 = pd.read_csv('/content/drive/MyDrive/hateandtoxic/samplehatetoxic_2021.csv')\n",
        "samplehatetoxic_2223 = pd.read_csv('/content/drive/MyDrive/hateandtoxic/samplehatetoxic_2223.csv')"
      ],
      "metadata": {
        "id": "YOpl8yeTSYc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out rows where hate_label is 'HATE' or toxic_label is 'toxic'\n",
        "hatetoxic_2021 = samplehatetoxic_2021[(samplehatetoxic_2021['hate_label'] == 'HATE') | (samplehatetoxic_2021['toxic_label'] == 'toxic')]\n",
        "hatetoxic_2223 = samplehatetoxic_2223[(samplehatetoxic_2223['hate_label'] == 'HATE') | (samplehatetoxic_2223['toxic_label'] == 'toxic')]"
      ],
      "metadata": {
        "id": "8Q0O8zNLSxww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the file paths in your Google Drive\n",
        "hatetoxic_2021_path = '/content/drive/My Drive/hateandtoxic/hatetoxic_2021.csv'\n",
        "hatetoxic_2223_path = '/content/drive/My Drive/hateandtoxic/hatetoxic_2223.csv'\n",
        "\n",
        "# Save the DataFrames as CSV files\n",
        "hatetoxic_2021.to_csv(hatetoxic_2021_path, index=False)\n",
        "hatetoxic_2223.to_csv(hatetoxic_2223_path, index=False)"
      ],
      "metadata": {
        "id": "-kVWFt1MFOU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hatetoxic_2021 = pd.read_csv('/content/drive/MyDrive/hateandtoxic/hatetoxic_2021.csv')\n",
        "hatetoxic_2223 = pd.read_csv('/content/drive/MyDrive/hateandtoxic/hatetoxic_2223.csv')"
      ],
      "metadata": {
        "id": "Re-N1cvBFmjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "9887D3HMVINP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rememoval of commonly used words in Singapore\n",
        "stop_words.update(['lah', 'lor', 'leh', 'liao', 'meh', 'mah', 'ah', 'hor', 'wah', 'lah','la', 'hor', 'sian', 'see', 'yeah', 'ya', 'yah', 'le','ba', 'bah', 'haha', 'bro','want','wants'\n",
        "                    'already', 'also', 'one', 'can', 'cannot', 'got', 'like', 'really', 'lol', 'lmao', 'yes', 'no', 'eh' ,'ah', 'omg', 'go', 'get', 'must', 'man','one', 'know', 'need'\n",
        "                    'sia', 'walao', 'siao', 'alamak', 'confirm', 'makan','aiyah', 'aiyo', 'aiyah','sure','even','probably','think', 'ok', 'okay', 'tbh','make', 'n',\n",
        "                    'still', 'maybe','said','you know', 'i mean', 'like that', 'do not know', 'not sure', 'of course', 'how come','always','alway','say', 'damn','give', 'going', 'take', 'took',\n",
        "                   'would','should','could','thing', 'right','oh','ah','as','sia'])"
      ],
      "metadata": {
        "id": "SkmOpBc-QWbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    # Tokenization\n",
        "    words = word_tokenize(text.lower())\n",
        "    # Remove punctuation and non-alphabetic tokens\n",
        "    words = [word for word in words if word.isalpha()]\n",
        "    # Stopword removal and lemmatization\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "\n",
        "    return ' '.join(words)"
      ],
      "metadata": {
        "id": "3-_kTYsHY3SM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing stopwords and punctuation\n",
        "hatetoxic_preprocessed_2021 = [preprocess(text) for text in hatetoxic_2021['text']]\n",
        "hatetoxic_preprocessed_2223 = [preprocess(text) for text in hatetoxic_2223['text']]"
      ],
      "metadata": {
        "id": "att9vU8WY43r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all entries to string\n",
        "hatetoxic_preprocessed_2021 = [str(text) for text in hatetoxic_preprocessed_2021]\n",
        "hatetoxic_preprocessed_2223 = [str(text) for text in hatetoxic_preprocessed_2223]"
      ],
      "metadata": {
        "id": "Y-VneYUh30W4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hatetoxic_2021.loc[:, 'text without punctuation and stopword'] = hatetoxic_preprocessed_2021\n",
        "hatetoxic_2223.loc[:, 'text without punctuation and stopword'] = hatetoxic_preprocessed_2223"
      ],
      "metadata": {
        "id": "cMuo9Ay5ZRAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking and removing empty rows in 'text without punctuation and stopword' column after cleaning\n",
        "hatetoxic_2021 = hatetoxic_2021[\n",
        "    (hatetoxic_2021['text without punctuation and stopword'].str.strip() != '') &  # Condition to check for non-empty strings\n",
        "    (hatetoxic_2021['text without punctuation and stopword'].notna())            # Condition to check for non-NaN values\n",
        "]\n",
        "\n",
        "\n",
        "hatetoxic_2223 = hatetoxic_2223[\n",
        "    (hatetoxic_2223['text without punctuation and stopword'].str.strip() != '') &  # Condition to check for non-empty strings\n",
        "    (hatetoxic_2223['text without punctuation and stopword'].notna())            # Condition to check for non-NaN values\n",
        "]"
      ],
      "metadata": {
        "id": "TBNqjjuz4Ymb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/Data Sample/hatetoxic_2021.csv'\n",
        "hatetoxic_2223.to_csv(file_path, index=False)"
      ],
      "metadata": {
        "id": "D9Q9uEtPPEZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/Data Sample/hatetoxic_2223.csv'\n",
        "hatetoxic_2223.to_csv(file_path, index=False)"
      ],
      "metadata": {
        "id": "OQQ7RYNZdyJm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}