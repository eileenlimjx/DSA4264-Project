{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "zoKsScEBa_J7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJQ-PmbNMMQL",
        "outputId": "db7028be-cc15-41eb-d721-1f53c8e44d12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "oNn06-zYMjW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_2021 = pd.read_csv('/content/drive/MyDrive/Data Sample/sample_2021.csv')\n",
        "sample_2223 = pd.read_csv('/content/drive/MyDrive/Data Sample/sample_2223.csv')"
      ],
      "metadata": {
        "id": "e1bs7lsmMiim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Assuming sample_2021 is a pandas DataFrame with a column 'text'\n",
        "dataset_2021 = Dataset.from_pandas(sample_2021)\n",
        "dataset_2223 = Dataset.from_pandas(sample_2223)"
      ],
      "metadata": {
        "id": "At_5SOdRGAyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Roberta Hate Model**"
      ],
      "metadata": {
        "id": "oiqoYiRO2kQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the hate speech detection pipeline with the RoBERTa model\n",
        "model_path = 'cardiffnlp/twitter-roberta-base-hate-latest'\n",
        "hate_classifier = pipeline(\"text-classification\", model=model_path, tokenizer=model_path, device=0)"
      ],
      "metadata": {
        "id": "8Xjc_UGyfr4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "# Assuming sample_2021 is a pandas DataFrame with a column 'text'\n",
        "dataset_2021 = Dataset.from_pandas(sample_2021)\n",
        "dataset_2223 = Dataset.from_pandas(sample_2223)"
      ],
      "metadata": {
        "id": "s7in4MZcadAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to classify hate speech\n",
        "def classify_hate(batch):\n",
        "    outputs = hate_classifier(batch['text'], truncation=True)\n",
        "    batch['label'] = [output['label'] for output in outputs]\n",
        "    batch['score'] = [output['score'] for output in outputs]\n",
        "    return batch"
      ],
      "metadata": {
        "id": "cAWFmjjM17qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the dataset in batches\n",
        "batch_size = 150\n",
        "dataset_2021 = dataset_2021.map(classify_hate, batched=True, batch_size=batch_size)\n",
        "\n",
        "# Extract the results\n",
        "hatelabel_2021 = dataset_2021['label']\n",
        "hatescore_2021 = dataset_2021['score']\n"
      ],
      "metadata": {
        "id": "RC0XciP2bGaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the dataset in batches\n",
        "batch_size = 150\n",
        "dataset_2223 = dataset_2223.map(classify_hate, batched=True, batch_size=batch_size)\n",
        "\n",
        "# Extract the results\n",
        "hatelabel_2223 = dataset_2223['label']\n",
        "hatescore_2223 = dataset_2223['score']"
      ],
      "metadata": {
        "id": "lWGLXHd_1O_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Roberta Toxicity**"
      ],
      "metadata": {
        "id": "bW2cq-gm2VPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "model_path = \"s-nlp/roberta_toxicity_classifier\"\n",
        "toxicity_classifier = pipeline(\"text-classification\", model=model_path, tokenizer=model_path, device = 0)"
      ],
      "metadata": {
        "id": "JiQidLdsRZ1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_toxic(batch):\n",
        "    outputs = toxicity_classifier(batch['text'], truncation=True)\n",
        "    batch['label'] = [output['label'] for output in outputs]\n",
        "    batch['score'] = [output['score'] for output in outputs]\n",
        "    return batch"
      ],
      "metadata": {
        "id": "n6zfF-u0GECd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the dataset in batches\n",
        "batch_size = 150\n",
        "dataset_2021 = dataset_2021.map(classify_toxic, batched=True, batch_size=batch_size)\n",
        "\n",
        "# Extract the results\n",
        "toxiclabel_2021 = dataset_2021['label']\n",
        "toxicscore_2021 = dataset_2021['score']"
      ],
      "metadata": {
        "id": "4X9zuAkEGJsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the dataset in batches\n",
        "batch_size = 150\n",
        "dataset_2223 = dataset_2223.map(classify_toxic, batched=True, batch_size=batch_size)\n",
        "\n",
        "# Extract the results\n",
        "toxiclabel_2223 = dataset_2223['label']\n",
        "toxicscore_2223 = dataset_2223['score']"
      ],
      "metadata": {
        "id": "_u72HNPtHpWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Saving files into Drive**"
      ],
      "metadata": {
        "id": "MqF6eDF3fdZv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2021 data"
      ],
      "metadata": {
        "id": "2iP2rIxSf6aY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_2021['hate_label'] = hatelabel_2021\n",
        "sample_2021['toxic_label'] = toxiclabel_2021"
      ],
      "metadata": {
        "id": "ShKEdy2FdbSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming sample_2021 is your DataFrame\n",
        "file_path = '/content/drive/My Drive/hateandtoxic/samplehatetoxic_2021.csv'\n",
        "sample_2021.to_csv(file_path, index=False)  # Save the DataFrame to CSV"
      ],
      "metadata": {
        "id": "RFGt7dvZfZJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2223 data"
      ],
      "metadata": {
        "id": "-9a-LsMwf_CO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_2223['hate_label'] = hatelabel_2223\n",
        "sample_2223['toxic_label'] = toxiclabel_2223"
      ],
      "metadata": {
        "id": "ZEu0GajZfsAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming sample_2021 is your DataFrame\n",
        "file_path = '/content/drive/My Drive/hateandtoxic/samplehatetoxic_2223.csv'\n",
        "sample_2223.to_csv(file_path, index=False)  # Save the DataFrame to CSV"
      ],
      "metadata": {
        "id": "CFi9B5JifxMK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}